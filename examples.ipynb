{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6e8574",
   "metadata": {},
   "source": [
    "# Experiments with the Interaction Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51bb446",
   "metadata": {},
   "source": [
    "In this notebook, we see how the _interaction decomposition_ from [Convy and Whaley (2023)](https://iopscience.iop.org/article/10.1088/2632-2153/aca271) can be used for tensor network machine learning. The focus here will be on key procedural steps and examples, with the bulk of the working code being imported from local modules `data.py`, `models.py`, `factor.py`, and `cnn.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b6119",
   "metadata": {},
   "source": [
    "## A brief introduction to tensor network machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1637d4d",
   "metadata": {},
   "source": [
    "Tensor network machine learning is a fairly new area of ML research, in which a model is constructed out of a network of parameterized tensors. A _tensor_ is best understood as a multidimensional array of numbers, which generalizes the matrix and vector objects from linear algebra. The number of dimensions in a tensor is referred to as its _order_, such that a vector is a first-order (1D) tensor and a matrix is a second-order (2D) tensor. Machine learning models can be built by joining multiple tensors together in a graph or network, with each edge of the graph denoting the multiplication of the tensors on the connected nodes.\n",
    "\n",
    "While a detailied description of tensor network machine learning is beyond the scope of this notebook, the basic pipeline can be summarized in the following steps:\n",
    "\n",
    "1. Implicitly transform the feature vectors of the target dataset into elements of an exponentially large space\n",
    "2. Multiply these transformed samples by the tensors in the network which parameterize the model\n",
    "3. Optimize the elements of those tensors using the output of the tensor multiplication\n",
    "4. Repeat steps 1-3 until the loss function converges.\n",
    "\n",
    "In practice, we can implement these steps easily using the Keras API in TensorFlow (or another ML package) with custom layers for steps 1 and 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42911a",
   "metadata": {},
   "source": [
    "### An example tensor network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d67e7",
   "metadata": {},
   "source": [
    "In the following code block, we can see a custom Keras layer that implements the first step in our procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Pow_Feat_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_power, dtype = \"float32\"):\n",
    "        super().__init__(dtype = dtype)\n",
    "        self.powers = tf.range(max_power + 1, dtype = dtype)[None, None]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = inputs[..., None] ** self.powers\n",
    "        return output   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5795981",
   "metadata": {},
   "source": [
    "This class inherits from the Keras `Layer` class, and operates by expanding the $n \\times m$ batch of feature vectors into an $n\\times m \\times p + 1$ batch of feature matrices. For a given sample matrix, the $i$th row has the form $[1, x_i, x^2_i,...,x^p_i]$, where $x_i$ is the $i$th data feature and $p$ is the maximum power. Implicitly, we view the the sample matrix as representing a tensor formed from the tensor product of each row, which would be an element of a massive $p^m$-dimensional tensor space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73b41c",
   "metadata": {},
   "source": [
    "For step 2, can implement another custom Keras layer that carries out the tensor multplication of our network with the output from `Pow_Feat_Layer`. The following code block provides an example of such a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30474789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MPS_Layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, bond_dim, num_classes, dtype = \"float32\"):\n",
    "        super().__init__(dtype = dtype)\n",
    "        self.bond_dim = bond_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.set_decomp(False)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, num_sites, phy_dim) = input_shape[:3]\n",
    "        self.num_sites = num_sites\n",
    "        self.split = tf.Variable(num_sites // 2, trainable = False) # The output is placed in the middle\n",
    "        self.matrix_weights = self.add_weight(\"matrix_weights\",\n",
    "            [phy_dim, num_sites, self.bond_dim, self.bond_dim], self.dtype, self.initializer)\n",
    "        self.middle = self.add_weight(\"middle\", # This tensor is the output component of the network\n",
    "            [self.num_classes, self.bond_dim, self.bond_dim], self.dtype, self.middle_initializer)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        # This function generates a prediction on the passed input batch.\n",
    "\n",
    "        split_data = tf.concat([inputs[:, self.split:], inputs[:, :self.split]], 1)\n",
    "        matrices = tf.einsum(\"nij,jikl->inkl\", split_data, self.matrix_weights)\n",
    "        matrix_prod = self.reduction(matrices)\n",
    "        outputs = tf.einsum(\"nkl,olk->no\", matrix_prod, self.middle)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def initializer(shape, dtype):\n",
    "\n",
    "        # This function initializes the component tensors of the network.\n",
    "        # The tensors need to be initialized such that they basically act \n",
    "        # like the identity.\n",
    "        \n",
    "        (phys_dim, num_sites, bond_dim, bond_dim) = shape\n",
    "        bias = tf.tile(tf.eye(bond_dim, dtype = dtype)[None, None], (1, num_sites, 1, 1))\n",
    "        kernel = tf.random.normal([phys_dim - 1, num_sites, bond_dim, bond_dim], 0, 1e-2, dtype)\n",
    "        weights = tf.concat([bias, kernel], 0)\n",
    "        return weights\n",
    "\n",
    "    @staticmethod\n",
    "    def middle_initializer(shape, dtype):\n",
    "        \n",
    "        # This function initializes the output component tensor.\n",
    "\n",
    "        (num_sites, bond_dim, bond_dim) = shape\n",
    "        weights = tf.tile([tf.eye(bond_dim, dtype = dtype)], (num_sites, 1, 1))\n",
    "        noised = weights + tf.random.normal(weights.shape, 0, 1e-2, dtype = dtype)\n",
    "        return noised\n",
    "\n",
    "    @staticmethod\n",
    "    def reduction(tensor):\n",
    "\n",
    "        # This function performs an efficient contraction of the MPS\n",
    "        # component matrices generated by contraction with the data\n",
    "        # vectors.\n",
    "\n",
    "        size = int(tensor.shape[0])\n",
    "        while size > 1:\n",
    "            half_size = size // 2\n",
    "            nice_size = 2 * half_size\n",
    "            leftover = tensor[nice_size:]\n",
    "            tensor = tf.matmul(tensor[0:nice_size:2], tensor[1:nice_size:2])\n",
    "            tensor = tf.concat([tensor, leftover], axis = 0)\n",
    "            size = half_size + int(size % 2 == 1)\n",
    "        return tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b2cae",
   "metadata": {},
   "source": [
    "As before, this layer inherits from the Keras `Layer` base class, and overloads the `build` and `call` methods. The `self.matrix_weights` array holds the parameters of the tensor network model, with each slice along the first dimension corresponding to a third-order tensor on one of the graph nodes. There are many different kinds of tensor networks, with this layer implementing a matrix product state (MPS) architecture. Note that custom initialization functions are needed in order to generate a numerically-stable output, and that the tensor multiplication can be performed efficiently in parallel. Once an output is generated, the parameters in `self.matrix_weights` (and `self.middle`) can be optimized in step 3 using stochastic gradient descent as would be done for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37588072",
   "metadata": {},
   "source": [
    "## The interaction decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f32d4",
   "metadata": {},
   "source": [
    "For a tensor network regression model using `Pow_Feat_layer` with $p = 1$ (which is most common), the prediction $y$ for sample vector $\\vec{x}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "y = w_0 + \\sum^m_{i=1}w_ix_i + \\sum^{m-1}_{i=1}\\sum^m_{j = i+1}w_{ij}x_ix_j + ... + w_{1,2,...,m}x_1x_2\\cdots x_m,\n",
    "\\end{equation}\n",
    "\n",
    "which is linear regression (with coefficients $w$ generated by the tensor multiplication) on every possible product of the original features. In an interaction decomposition, we explicity separate out contributions to $y$ based on the number of features multiplied together in the regressors. This _interaction degree_ ranges from 0 in the bias term $w_0$ to $m$ in the term $w_{1,2,...,m}x_1x_2\\cdots x_m$ which is the product of all features. Once the different interactions are disentangled, we can analyze their properties and modify their values individually.\n",
    "\n",
    "It turns out that the interaction decomposition can be implemented in a fairly straightforward manner by tweaking how the tensor operations are performed. The following code block performs an interaction decomposition for the MPS model shown previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280411e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def decomp(self, inputs, indices, **kwargs):\n",
    "    max_order = indices[-1].shape[0]\n",
    "    split_data = tf.concat([inputs[:, self.split:], inputs[:, :self.split]], 1)\n",
    "    order_matrices = tf.einsum(\"nsrj,jskl->rnskl\", split_data, self.matrix_weights)\n",
    "    cuml = order_matrices[:, :, 0]\n",
    "    for i in range(1, self.num_sites):\n",
    "        order_matrix = order_matrices[:, :, i]\n",
    "        contract = tf.einsum(\"rnkl,qnlm->qrnkm\", cuml, order_matrix)\n",
    "        combined = contract[0, 1:] + contract[1, :-1]\n",
    "        cuml = tf.concat([contract[0, :1], combined, contract[1, -1:]], 0)[:max_order]\n",
    "    order_output = tf.einsum(\"rnlm,oml->nor\", cuml, self.middle)\n",
    "    return order_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25165ac",
   "metadata": {},
   "source": [
    "The key modification is that each tensor is given an extra dimension which separates the different interaction degrees. When two tensors are multiplied together, the slices along this extra dimension are matched up and summed together to preserve the interaction degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce989b5",
   "metadata": {},
   "source": [
    "## Numerical experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa271b7",
   "metadata": {},
   "source": [
    "We will now perform numerical tests using the interaction decomposition on two different tensor network models: the MPS introduced previously, and a binary tree network called a TTN. We will train these models to classify images from the MNIST and Fashion MNIST datasets, though we will shrink them down from $28 \\times 28$ to $8 \\times 8$ to speed up our computations. The following code retrieves and plots a pair of example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_example_image(fashion = False):\n",
    "    if fashion:\n",
    "        image = tf.keras.datasets.fashion_mnist.load_data()[0][0][1]\n",
    "    else:\n",
    "        image = tf.keras.datasets.mnist.load_data()[0][0][1]\n",
    "    image = image / 255\n",
    "    image = image[4:24, 4:24] # Remove black border before resizing\n",
    "    image = transform.resize(image, (8, 8)) - 0.5\n",
    "    return image\n",
    "\n",
    "mnist_image = get_example_image()\n",
    "fashion_image = get_example_image(fashion = True)\n",
    "\n",
    "(fig, axes) = plt.subplots(1, 2)\n",
    "axes[0].imshow(mnist_image, cmap = \"gray\")\n",
    "axes[1].imshow(fashion_image, cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9730d20",
   "metadata": {},
   "source": [
    "Note that both images are largely recognizable after being shrunk, though this can vary depending on the image for Fashion MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c9517",
   "metadata": {},
   "source": [
    "### CNN baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886f97f",
   "metadata": {},
   "source": [
    "To get a sense for how difficult these two transformed datasets are to classify, we can run an experiment using an Inception-based CNN classifier as a state-of-the-art comparison point. The following code imports the model architecture from `cnn.py` and then trains it for 50 epochs on either MNIST or Fashion MNIST as specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a821c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import data\n",
    "import cnn\n",
    "\n",
    "fashion = False\n",
    "\n",
    "data_function = data.get_fashion_data if fashion else data.get_mnist_data\n",
    "((x_train, y_train), (x_test, y_test)) = data_function(border = False, size = (8, 8))\n",
    "x_train = tf.reshape(x_train, [-1, 8, 8])\n",
    "x_test = tf.reshape(x_test, [-1, 8, 8])\n",
    "x_train = tf.tile(x_train[..., None], (1, 1, 1, 3))\n",
    "x_test = tf.tile(x_test[..., None], (1, 1, 1, 3))\n",
    "\n",
    "model = cnn.build_inception(x_train)\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "                optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.006, beta_1 = 0.49, beta_2 = 0.999),\n",
    "                metrics = ['accuracy'])\n",
    "model.fit(x_train, y_train, 64, 50, validation_split = 1/6, verbose = 1)\n",
    "score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b108",
   "metadata": {},
   "source": [
    "After training, we find accuracies of 99.2-99.3% on MNIST and accuracies of 86.4-86.8% on Fashion MNIST. This shows that the $8 \\times 8$ Fashion MNIST images are significantly harder to classify than the full $28 \\times 28$ images, while MNIST does not seem to have been affected much by the resizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a5c44",
   "metadata": {},
   "source": [
    "### MPS and TTN models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679393d",
   "metadata": {},
   "source": [
    "The next models that we will test are the regular MPS and TTN tensor network models. For this initial set of experiments, we will perform the tensor operations normally during training, and then carry out the interaction decomposition at the end on the test dataset. The following code imports the tensor network models from `models.py`, trains them, and then saves the results. There are four different training combinations that can be selected, using either an MPS or TTN model and either MNIST or Fashion MNIST for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46018990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import data\n",
    "import factor\n",
    "import models\n",
    "\n",
    "model_name = \"ttn\" # either \"mps\" or \"ttn\"\n",
    "dataset = \"fashion\" # either \"mnist\" or \"fashion\"\n",
    "save = True\n",
    "\n",
    "((train_x, train_y), (test_x, test_y), num_classes) = data.get_dataset(\n",
    "        dataset, size = (8, 8), border = False)\n",
    "model = models.get_model(model_name, num_classes, dtype = \"float64\", bond_dim = 20)\n",
    "model.compile(loss = \"mse\",\n",
    "    optimizer = tf.keras.optimizers.RMSprop(1e-3),\n",
    "    metrics = ['accuracy'],\n",
    "    run_eagerly = False)\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    validation_split = 1/6)\n",
    "\n",
    "score = model.evaluate(test_x, test_y, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "if save:\n",
    "    save_path = f\"saved/{model_name}_{dataset}_test\"\n",
    "    model_string = f\"/{model_name}_{num_classes}_20_float64\"\n",
    "    model.save_weights(save_path + model_string, save_format = \"tf\")\n",
    "    print(\"Computing interaction degrees...\")\n",
    "    factor.factorize(save_path, (test_x, test_y))\n",
    "    print(\"\\nComplete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a76b5",
   "metadata": {},
   "source": [
    "The contributions from the different interaction degrees for each test sample are saved in the `factor.npz` compressed NumPy archive. These results can be aggregated into a more intepretable form by the following two functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def get_labeled_order_acc(path):\n",
    "    data = np.load(path + \"/factors.npz\")\n",
    "    pred = np.argmax(data[\"results\"], 1)\n",
    "    labels = np.argmax(data[\"labels\"], -1)[:, None]\n",
    "    accs = (pred == labels).mean(0)\n",
    "    return tf.convert_to_tensor(accs)\n",
    "\n",
    "def get_labeled_cuml_acc(path):\n",
    "    data = np.load(path + \"/factors.npz\")\n",
    "    labels = np.argmax(data[\"labels\"], -1)[:, None]\n",
    "    cum_factors = np.cumsum(data[\"results\"], -1)\n",
    "    pred = np.argmax(cum_factors, 1)\n",
    "    accs = (pred == labels).mean(0)\n",
    "    return tf.convert_to_tensor(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751e291",
   "metadata": {},
   "source": [
    "These functions compute two different kinds of accuracy values with respect to the interaction decomposition. To get the accuracy of the interaction degrees individually, we use `get_labeled_order_acc` to compare the argmax for the prediction with the true label for each degree. To get the accuracy of the sum of all interaction degrees less than or equal to a given value, we use `get_labeled_cuml_acc` which sums the different predictions before taking the argmax. When these accuracy values are averaged across ten different model instantiations, we get the following plot:\n",
    "\n",
    "<img src=\"figures/accs.png\"  width=\"850\">\n",
    "\n",
    "where \"TR\" stands for \"tensor ring\", which is a more precise term for our MPS model. The solid lines mark the cumulative accuracy of the interaction degrees, while the scatter plot points mark the individual accuracies of each degree. These plots show that the cumulative accuracies require half or more of the interaction degrees before converging to to the final accuracy value, while the individual accuracies all have very poor performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a8bab",
   "metadata": {},
   "source": [
    "### Interaction-constrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57400cfc",
   "metadata": {},
   "source": [
    "In this final section, we consider tensor network models which make predictions using only a subset of interaction degrees. In particular, we consider models which use only the $j$th interaction degree (the _degree-j_ models), or which use the cumulative sum of interaction degree contributions up to the $j$th degree (the _cumulative-j_ models). We can train these models in precisely the same manner as the full models in the previous section, using only a slighlty modified code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517742d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import data\n",
    "import factor\n",
    "import models\n",
    "\n",
    "model_name = \"ttn\" # either \"mps\" or \"ttn\"\n",
    "dataset = \"mnist\" # either \"mnist\" or \"fashion\"\n",
    "save = True\n",
    "\n",
    "((train_x, train_y), (test_x, test_y), num_classes) = data.get_dataset(\n",
    "        dataset, size = (8, 8), border = False)\n",
    "model = models.get_model(model_name, num_classes, dtype = \"float64\", bond_dim = 20)\n",
    "\n",
    "max_order = [2] # max_order = j gives a cumulative-j model, max_order = [j] gives a degree-j model\n",
    "model.set_output(True, True)\n",
    "model.set_order(max_order)\n",
    "\n",
    "model.compile(loss = \"mse\",\n",
    "    optimizer = tf.keras.optimizers.RMSprop(1e-3),\n",
    "    metrics = ['accuracy'],\n",
    "    run_eagerly = False)\n",
    "model.fit(train_x, train_y,\n",
    "    batch_size = 128,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    validation_split = 1/6)\n",
    "\n",
    "score = model.evaluate(test_x, test_y, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "if save:\n",
    "    decomp = f\"cuml-{max_order}\" if isinstance(max_order, int) else f\"deg-{max_order[0]}\"\n",
    "    save_path = f\"saved/{model_name}_{dataset}_{decomp}\"\n",
    "    model_string = f\"/{model_name}_{num_classes}_20_float64\"\n",
    "    model.save_weights(save_path + model_string, save_format = \"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bb440",
   "metadata": {},
   "source": [
    "If we train models cumlative-1 through cumulative-10 and degree-1 through degree-10, we can create a plot of their average accuracies versus the interaction degree accuracies in the previous section:\n",
    "\n",
    "<img src=\"figures/trained.png\"  width=\"850\">\n",
    "\n",
    "where the previously plotted results are shown in black. From these new set of plots, we can see that the interaction-constrained models are often as effective as the full models, despite containing far fewer regressors. This suggests that the full tensor network models are using their low-degree regressors in a highly inefficient manner, since the corresponding interaction degree accuracies plotted in black are much lower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
